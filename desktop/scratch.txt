#[tauri::command]
async fn get_inference(params: TParamIteration) -> Result<String, Error> {
    // println!("{:?}", params);
    println!("model {}", params.model);
    println!("prompt {}", params.prompt);

    let ollama = Ollama::default();
    let res = ollama
        .generate(GenerationRequest::new(params.model, params.prompt))
        .await?;

    println!(">>> {:?}", res.to_owned());

    // if let Ok(res) = res {
    //     println!(">>> {:?}", res.to_owned());
    // }

    // if let Err(res) = res {
    //     println!(">>> {:?}", res.to_owned());
    // }

    Ok("done".to_string())

    // let ollama = Ollama::default();
    // let gen_req = GenerationRequest::new(params.model, params.prompt);

    // let res = ollama.generate(gen_req).await.unwrap();

    // println!("res is");
    // println!("{:?}", res);
    // if res. {
    //     println!("Error: {:?}", res);
    // } else {
    //     println!("Success: {:?}", res)
    // }

    // match ollama.generate(gen_req).await? {
    //     Genera(res) => {
    //         println!("it works");
    //         return Ok(res.response.to_string());
    //     }
    //     Err(err) => {
    //         // Return a descriptive error message if listing fails
    //         println!("Error: {}", err);
    //         return Err(Error::Ollama(
    //             format!("Failed to generate completion: {}", err).into(),
    //         ));
    //     }
    // };

    //     .generate(GenerationRequest::new(params.model, params.prompt))
    //     .await
    // {
    //     Ok(res) => {
    //         println!("it works");
    //     }
    //     Err(err) => {
    //         // Return a descriptive error message if listing fails
    //         println!("Error: {}", err);
    //         return Err(Error::Ollama(
    //             format!("Failed to list local models: {}", err).into(),
    //         ));
    //     }
    // };

    // let ollama = Ollama::default();
    // let res = ollama
    //     .generate(GenerationRequest::new(params.model, params.prompt).stream(false))
    //     .await
    //     .unwrap();
}



#[tauri::command]
async fn get_models() -> Result<Vec<String>, Error> {
    let ollama = Ollama::default();

    // let models = match ollama.list_local_models().await {
    //     Ok(models) => models,
    //     Err(err) => {
    //         // Return a descriptive error message if listing fails
    //         println!("Error: {}", err);
    //         return Error::Ollama(format!("Failed to list local models: {}", err));
    //     }
    // };

    let models = ollama.list_local_models().await?;

    let model_list: Vec<String> = models.into_iter().map(|model| model.name).collect();
    Ok(model_list)
}



Error when Ollama is down
Error: String error: error sending request for url (http://127.0.0.1:11434/api/generate): error trying to connect: tcp connect error: Connection refused (os error 111)



#[tauri::command]
async fn get_inference(params: TParamIteration) -> Result<String, Error> {
    let ollama = Ollama::default();
    println!("get_inference was invoked..");
    let res = match ollama
        .generate(GenerationRequest::new(params.model, params.prompt))
        .await
    {
        Ok(value) => value,
        Err(err) => {
            println!("Error: {}", err.to_string());
            return Err(Error::StringError(err.to_string()));
        }
    };
    dbg!(res);
    Ok("done".to_string())
}


// Shape of response

[src/main.rs:96] res = GenerationResponse {
    model: "dolphin-mistral:v2.6",
    created_at: "2024-02-08T19:54:00.324376786Z",
    response: "Ola! How can I help you today?",
    done: true,
    final_data: Some(
        GenerationFinalResponseData {
            context: GenerationContext(
                [
                    32000,
                    6574,
                    13,
                    1976,
                    460,
                    15052,
                    721,
                    262,
                    28725,
                    264,
                    10865,
                    16107,
                    13892,
                    28723,
                    13,
                    2,
                    13,
                    32000,
                    1838,
                    13,
                    28762,
                    28710,
                    2,
                    13,
                    32000,
                    489,
                    11143,
                    13,
                    28762,
                    2220,
                    28808,
                    1602,
                    541,
                    315,
                    1316,
                    368,
                    3154,
                    28804,
                ],
            ),
            total_duration: 4287291844,
            prompt_eval_count: 29,
            prompt_eval_duration: 2117403000,
            eval_count: 11,
            eval_duration: 1180868000,
        },
    ),
}
